apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "liferay-helm.fullname" . }}
  labels:
    {{- include "liferay-helm.labels" . | nindent 4 }}
  {{- with .Values.configmap.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
data:
  {{- if not .Values.configmap.overrideDefaults }}
  portal-ext.properties: |
    include-and-override=portal-custom.properties

    #
    # PostgreSQL
    #
    jdbc.default.driverClassName=org.postgresql.Driver
    {{- with .Values.postgres.config }}
    jdbc.default.url=jdbc:postgresql://{{ default (printf "%s-postgres" (include "liferay-helm.fullname" $)) .host }}:{{ .port }}/{{ .database }}?{{ .parameters }}
    jdbc.default.username={{ .user }}
    jdbc.default.password={{ .password }}
    {{- end }}

    #
    # Amazon S3 (uses MinIO when `s3.internal.enabled=true`)
    #
    dl.store.impl=com.liferay.portal.store.s3.S3Store
    {{- with .Values.s3.config }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_accessKey={{ .user | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_bucketName={{ .buckets | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_connectionProtocol={{ .scheme | upper | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_connectionTimeout=i{{ .connectionTimeout | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_corePoolSize=i{{ .corePoolSize | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_httpClientMaxConnections=i{{ .httpClientMaxConnections | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_httpClientMaxErrorRetry=B{{ .httpClientMaxErrorRetry | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3Endpoint="{{ default (printf "%s-minio" (include "liferay-helm.fullname" $)) .host }}:{{ .ports.api }}"
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3PathStyle=B{{ .pathStyle | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3Region={{ .region | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_secretKey={{ .password | quote }}
    {{- end }}

    database.indexes.update.on.startup=true
    #index.on.startup=true
    notification.email.template.enabled=false
    passwords.default.policy.change.required={{ .Values.config.requiredPasswordChange }}
    passwords.default.policy.lockout.duration=3600
    passwords.default.policy.lockout=true
    passwords.default.policy.max.failure=20
    passwords.default.policy.reset.failure.count=3600
    portal.instance.protocol=http
    schema.module.build.auto.upgrade=true
    setup.wizard.enabled=false
    upgrade.database.auto.run=true
    upgrade.log.context.enabled=true
    upgrade.report.dir=${liferay.home}/reports
    upgrade.report.dl.storage.size.timeout=0
    upgrade.report.enabled=true
    virtual.hosts.valid.hosts=*
    web.server.forwarded.host.enabled=true
    web.server.forwarded.protocol.enabled=true
    web.server.http.port=80
    web.server.https.port=443
    web.server.protocol=http

    default.admin.password={{ .Values.config.password }}
    default.admin.screen.name={{ .Values.config.user }}
    default.admin.email.address.prefix={{ .Values.config.user }}
    company.default.name=Liferay DXP
    company.default.web.id={{ .Values.config.mainVirtualHost }}
    company.default.virtual.host.name={{ .Values.config.mainVirtualHost }}
    company.default.virtual.host.mail.domain={{ .Values.config.mainVirtualHost }}
    company.default.virtual.host.sync.on.startup=true

    module.framework.properties.org.apache.felix.configadmin.plugin.interpolation.secretsdir=/opt/liferay/osgi/configs,/var/run/secrets/kubernetes.io/serviceaccount

    configuration.override.com.liferay.portal.component.blacklist.internal.configuration.ComponentBlacklistConfiguration_blacklistComponentNames=[\
      "com.liferay.portal.store.s3.IBMS3Store"\
    ]

    # As a System admin, I would like to use Site/Instance OSGi configurations across different systems
    feature.flag.LPS-155284=true

    # SRE-5860 Enable TCP keep alive
    configuration.override.com.liferay.portal.http.internal.configuration.HttpConfiguration_tcpKeepAliveEnabled=B"true"

    # SRE-5749 enable FeatureFlag LPS-202104
    feature.flag.LPS-202104=true

    #
    # Clustering
    #
    {{- if  or (gt (int .Values.replicaCount) 1) .Values.autoscaling.enabled }}
    cluster.link.autodetect.address=
    cluster.link.channel.properties.control=/opt/liferay/unicast.xml
    cluster.link.channel.properties.transport.0=/opt/liferay/unicast.xml
    cluster.link.enabled=true
    {{- end }}

  com.liferay.redirect.internal.configuration.RedirectURLConfiguration.config: |
    securityMode="domain"

  com.liferay.portal.k8s.agent.configuration.PortalK8sAgentConfiguration.config: |
    apiServerHost="$[env:KUBERNETES_SERVICE_HOST]"
    apiServerPort="$[env:KUBERNETES_SERVICE_PORT]"
    apiServerSSL=b"true"
    caCertData="$[secret:ca.crt]"
    namespace="$[secret:namespace]"
    saToken="$[secret:token]"

  com.liferay.portal.search.elasticsearch7.configuration.ElasticsearchConfiguration.config: |
    {{- with .Values.elasticsearch.config }}
    clusterName={{ .clusterName | quote }}
    indexNamePrefix="liferay-"
    networkHostAddresses=["{{ default (printf "%s-elasticsearch" (include "liferay-helm.fullname" $)) .host }}:{{ .port }}"]
    operationMode="REMOTE"
    password={{ .password | quote }}
    {{- end }}

  {{- if  or (gt (int .Values.replicaCount) 1) .Values.autoscaling.enabled }}
  unicast.xml: |
    <!--
        TCP based stack, with flow control and message bundling. This is usually used when IP
        multicasting cannot be used in a network, e.g. because it is disabled (routers discard multicast).
        Note that TCP.bind_addr and TCPPING.initial_hosts should be set, possibly via system properties, e.g.
        -Djgroups.bind_addr=192.168.5.2 and -Djgroups.tcpping.initial_hosts=192.168.5.2[7800]
        author: Bela Ban
    -->
    <config
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
        xmlns="urn:org:jgroups"
        xsi:schemaLocation="urn:org:jgroups http://www.jgroups.org/schema/jgroups.xsd">

        <TCP
            bind_port="{{ .Values.service.clusterPort }}"
            max_bundle_size="64K"
            port_range="1"
            recv_buf_size="${tcp.recv_buf_size:5M}"
            send_buf_size="${tcp.send_buf_size:5M}"
            sock_conn_timeout="300"
            thread_pool.enabled="true"
            thread_pool.min_threads="2"
            thread_pool.max_threads="8"
            thread_pool.keep_alive_time="5000"/>

        <dns.DNS_PING
            dns_query="_cluster._tcp.{{ include "liferay-helm.fullname" . }}-headless"
            dns_record_type="SRV" />
        <MERGE3  min_interval="10000" max_interval="30000"/>
        <FD_SOCK/>
        <FD timeout="3000" max_tries="3" />
        <VERIFY_SUSPECT timeout="1500"  />
        <BARRIER />
        <pbcast.NAKACK2 use_mcast_xmit="false" discard_delivered_msgs="true"/>
        <UNICAST3 />
        <pbcast.STABLE
            stability_delay="1000" desired_avg_gossip="50000" max_bytes="4M"/>

        <pbcast.GMS
            print_local_addr="true" join_timeout="2000" view_bundling="true"/>

        <MFC max_credits="2M" min_threshold="0.4"/>
        <FRAG2 frag_size="60K"  />
        <!--RSVP resend_interval="2000" timeout="10000"/-->
        <pbcast.STATE_TRANSFER/>
    </config>
  {{- end }}

  {{- end }}

  {{- toYaml .Values.configmap.data | nindent 2 }}
