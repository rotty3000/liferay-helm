apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "liferay-helm.fullname" . }}
  labels:
    {{- include "liferay-helm.labels" . | nindent 4 }}
  {{- with .Values.configmap.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
data:
  portal-ext.properties: |
    include-and-override=portal-custom.properties

    #
    # PostgreSQL
    #
    jdbc.default.driverClassName=org.postgresql.Driver
    {{- with .Values.postgres.config }}
    jdbc.default.url=jdbc:postgresql://{{ default (printf "%s-postgres" (include "liferay-helm.fullname" $)) .host }}:{{ .port }}/{{ .database }}?{{ .parameters }}
    jdbc.default.username={{ .user }}
    jdbc.default.password={{ .password }}
    {{- end }}

    #
    # Amazon S3 (uses MinIO when `minio.internal.enabled=true`)
    #
    dl.store.impl=com.liferay.portal.store.s3.S3Store
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_accessKey={{ .Values.minio.config.user | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_bucketName={{ .Values.minio.config.buckets | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_connectionProtocol={{ .Values.minio.config.scheme | upper | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_connectionTimeout=i{{ .Values.minio.config.connectionTimeout | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_corePoolSize=i{{ .Values.minio.config.corePoolSize | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_httpClientMaxConnections=i{{ .Values.minio.config.httpClientMaxConnections | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_httpClientMaxErrorRetry=B{{ .Values.minio.config.httpClientMaxErrorRetry | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3Endpoint="{{ default (printf "%s-minio" (include "liferay-helm.fullname" $)) .Values.minio.config.host }}:{{ .Values.minio.config.ports.api }}"
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3PathStyle=B{{ .Values.minio.config.pathStyle | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_s3Region={{ .Values.minio.config.region | quote }}
    configuration.override.com.liferay.portal.store.s3.configuration.S3StoreConfiguration_secretKey={{ .Values.minio.config.password | quote }}

    database.indexes.update.on.startup=true
    #index.on.startup=true
    notification.email.template.enabled=false
    passwords.default.policy.change.required=false
    passwords.default.policy.lockout.duration=3600
    passwords.default.policy.lockout=true
    passwords.default.policy.max.failure=20
    passwords.default.policy.reset.failure.count=3600
    portal.instance.protocol=http
    schema.module.build.auto.upgrade=true
    setup.wizard.enabled=false
    upgrade.database.auto.run=true
    upgrade.log.context.enabled=true
    upgrade.report.dir=${liferay.home}/reports
    upgrade.report.dl.storage.size.timeout=0
    upgrade.report.enabled=true
    virtual.hosts.valid.hosts=*
    web.server.forwarded.host.enabled=true
    web.server.forwarded.protocol.enabled=true
    web.server.http.port=80
    web.server.https.port=443
    web.server.protocol=http

    company.default.name=Liferay DXP Main
    #company.default.web.id=main.dxp.docker.localhost
    company.default.virtual.host.name=main.dxp.docker.localhost
    company.default.virtual.host.mail.domain=main.dxp.docker.localhost
    company.default.virtual.host.sync.on.startup=true

    module.framework.properties.org.apache.felix.configadmin.plugin.interpolation.secretsdir=/opt/liferay/osgi/configs,/var/run/secrets/kubernetes.io/serviceaccount

    configuration.override.com.liferay.portal.component.blacklist.internal.configuration.ComponentBlacklistConfiguration_blacklistComponentNames=[\
      "com.liferay.portal.store.s3.IBMS3Store"\
    ]

    # As a System admin, I would like to use Site/Instance OSGi configurations across different systems
    feature.flag.LPS-155284=true

    # SRE-5860 Enable TCP keep alive
    configuration.override.com.liferay.portal.http.internal.configuration.HttpConfiguration_tcpKeepAliveEnabled=B"true"

    # SRE-5749 enable FeatureFlag LPS-202104
    feature.flag.LPS-202104=true

  com.liferay.portal.k8s.agent.configuration.PortalK8sAgentConfiguration.config: |
    apiServerHost="$[env:KUBERNETES_SERVICE_HOST]"
    apiServerPort="$[env:KUBERNETES_SERVICE_PORT]"
    apiServerSSL=b"true"
    caCertData="$[secret:ca.crt]"
    namespace="$[secret:namespace]"
    saToken="$[secret:token]"

  com.liferay.portal.search.elasticsearch7.configuration.ElasticsearchConfiguration.config: |
    {{- with .Values.elasticsearch.config }}
    clusterName="{{ .clusterName }}"
    # authenticationEnabled=b"true"
    indexNamePrefix="liferay-"
    networkHostAddresses=["{{ default (printf "%s-elasticsearch" (include "liferay-helm.fullname" $)) .host }}:{{ .port }}"]
    operationMode="REMOTE"
    password="{{ .password }}"
    {{- end }}

  {{- toYaml .Values.configmap.data | nindent 2 }}
